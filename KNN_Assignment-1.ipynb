{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00effccc-275b-4d43-8668-bff829afa42d",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?\n",
    "\n",
    "### Q2. How do you choose the value of K in KNN?\n",
    "\n",
    "### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "### Q4. How do you measure the performance of KNN?\n",
    "\n",
    "### Q5. What is the curse of dimensionality in KNN?\n",
    "\n",
    "### Q6. How do you handle missing values in KNN?\n",
    "\n",
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c047c0-94b5-4a51-8ba8-0de13dff3546",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dcc180-64e9-4087-9ddf-d1f879216eea",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64787126-a015-4bc3-a34d-d1af34fa867c",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) is a simple and widely used machine learning algorithm for classification and regression tasks. It is a type of instance-based, non-parametric, and lazy learning algorithm, which means it doesn't make any assumptions about the underlying data distribution and doesn't learn a model during training. Instead, KNN makes predictions by comparing a new, unseen data point to its K nearest neighbors in the training dataset.\n",
    "\n",
    "\n",
    "1. **Training**: In the training phase, the algorithm simply stores the feature vectors and their corresponding class labels from the training dataset.\n",
    "\n",
    "2. **Prediction for Classification**:\n",
    "   - Given a new data point that you want to classify, KNN calculates the distance (usually Euclidean distance, but other distance metrics can be used) between that data point and all the data points in the training dataset.\n",
    "   - It then selects the K nearest data points (neighbors) with the smallest distances.\n",
    "   - For classification, KNN takes a majority vote of the class labels of these K neighbors to determine the class of the new data point. The class that occurs most frequently among the neighbors is assigned to the new data point.\n",
    "\n",
    "3. **Prediction for Regression**:\n",
    "   - For regression tasks, KNN calculates the average (or weighted average) of the target values of the K nearest neighbors to predict the target value of the new data point.\n",
    "\n",
    "4. **Choosing the Value of K**: The choice of the value of K is a hyperparameter that can significantly affect the performance of the KNN algorithm. A small K value can make the algorithm sensitive to noise in the data, while a large K value can make the algorithm too biased and result in underfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a371cd6-081f-485c-84d1-b8e0d4830e5b",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the value of K in KNN?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22f8c13-025b-4ebc-be3e-f2d7703016ca",
   "metadata": {},
   "source": [
    "Choosing the value of K in the K-Nearest Neighbors (KNN) algorithm is a critical decision, as it can significantly impact the model's performance. The choice of K affects the balance between bias and variance in the model. Here are some common methods to choose an appropriate value for K:\n",
    "\n",
    "1. **Manual Tuning and Experimentation**:\n",
    "   - Start with a small value of K, e.g., K=1, and gradually increase it.\n",
    "   - Evaluate the model's performance (using metrics like accuracy for classification or mean squared error for regression) for different K values on a validation dataset or through cross-validation.\n",
    "   - Choose the K that provides the best balance between bias and variance, based on your evaluation metrics.\n",
    "\n",
    "2. **Square Root of the Number of Data Points**:\n",
    "   - A rule of thumb is to set K to the square root of the number of data points in your training dataset. This is a simple and quick way to choose a reasonable K value.\n",
    "\n",
    "\n",
    "3. **Use Cross-Validation**:\n",
    "   - Perform k-fold cross-validation on your training data for different K values. This helps you estimate how your model might perform on unseen data and select the K that minimizes cross-validation error.\n",
    "\n",
    "4. **Grid Search**:\n",
    "   - In some cases, you can use grid search along with cross-validation to systematically search for the best K value along with other hyperparameters. This approach is more computationally expensive but can lead to better results.\n",
    "\n",
    "5. **Domain Knowledge**:\n",
    "   - Consider the characteristics of your data and problem domain. Sometimes, domain knowledge can guide the choice of K. For example, if you know that the decision boundary is likely to be smooth, you might choose a larger K.\n",
    "\n",
    "6. **Elbow Method (for Error Rate)**:\n",
    "   - In classification problems, you can use the \"elbow method\" to select K by plotting the error rate (e.g., misclassification rate) as a function of K. The point where the error rate starts to stabilize or form an \"elbow\" is a good choice for K.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84119edf-a19b-4291-ac62-74f0c30859ee",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between KNN classifier and KNN regressor?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca3119-41c7-47fd-9d2b-a5e96fc5a863",
   "metadata": {},
   "source": [
    "1. **KNN Classifier**:\n",
    "   - **Problem Type**: KNN classifier is used for solving classification problems, where the goal is to categorize data points into predefined classes or categories. For example, it can be used to classify emails as spam or not spam, classify images of animals into different species, or determine whether a customer will buy a product or not.\n",
    "   - **Prediction Method**: KNN classifier makes predictions by assigning a class label to a data point based on the majority class among its K nearest neighbors. The class with the most occurrences among the K neighbors is the predicted class for the data point.\n",
    "   - **Output**: The output of a KNN classifier is a discrete class label.\n",
    "\n",
    "2. **KNN Regressor**:\n",
    "   - **Problem Type**: KNN regressor is used for solving regression problems, where the goal is to predict a continuous numeric value or a real number. For example, it can be used to predict a house's price based on its features, forecast the temperature, or estimate a person's age based on certain characteristics.\n",
    "   - **Prediction Method**: KNN regressor makes predictions by calculating the average (or weighted average) of the target values of its K nearest neighbors. The predicted value for a data point is a numeric value based on the mean or weighted mean of the target values of the K neighbors.\n",
    "   - **Output**: The output of a KNN regressor is a continuous numeric value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6faff0-35ae-4236-81be-d9c0632875b3",
   "metadata": {},
   "source": [
    "### Q4. How do you measure the performance of KNN?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0ae65d-c4ed-4d40-82b9-291bb40d1065",
   "metadata": {},
   "source": [
    "\n",
    "**KNN Classifier**:\n",
    "\n",
    "1. **Accuracy**: Accuracy is the most straightforward metric for classification. It measures the proportion of correctly classified instances out of the total instances. It's calculated as (Number of Correct Predictions) / (Total Number of Predictions).\n",
    "\n",
    "2. **Confusion Matrix**: A confusion matrix provides a more detailed view of the model's performance by showing the true positives, true negatives, false positives, and false negatives. From the confusion matrix, you can derive metrics like precision, recall, and F1-score.\n",
    "\n",
    "3. **Precision**: Precision measures the accuracy of positive predictions. It's calculated as (True Positives) / (True Positives + False Positives). It's useful when false positives are costly.\n",
    "\n",
    "4. **Recall (Sensitivity)**: Recall measures the ability of the model to correctly identify positive instances. It's calculated as (True Positives) / (True Positives + False Negatives). It's useful when false negatives are costly.\n",
    "\n",
    "5. **F1-Score**: The F1-Score is the harmonic mean of precision and recall, providing a balanced measure of a model's performance. It's calculated as 2 * (Precision * Recall) / (Precision + Recall).\n",
    "\n",
    "6. **ROC Curve and AUC**: Receiver Operating Characteristic (ROC) curves are useful for evaluating binary classification models. The Area Under the ROC Curve (AUC) quantifies the model's ability to discriminate between positive and negative classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59eda0-4200-4de2-83ba-421c4fe9d97a",
   "metadata": {},
   "source": [
    "### Q5. What is the curse of dimensionality in KNN?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a0485-1523-4cf9-b4ee-72534c55e1f9",
   "metadata": {},
   "source": [
    "- The dimensionality curse phenomenon states that in high dimensional spaces distances between nearest and farthest points from query points become almost equal. Therefore, nearest neighbor calculations cannot discriminate candidate points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d53eb92-f765-4699-9c48-670616f8cc24",
   "metadata": {},
   "source": [
    "- The curse of dimensionality in the k-nearest neighbor (kNN) algorithm refers to the increasing computational complexity and sparsity of data as the number of dimensions increases. This can lead to overfitting and poor performance of the algorithm.\n",
    "\n",
    "- The curse of dimensionality in the kNN context basically means that Euclidean distance is unhelpful in high dimensions because all vectors are almost equidistant to the search query vector.\n",
    "\n",
    "#### The curse of dimensionality can also be described as: \n",
    "- The size of the data space grows exponentially with the number of dimensions.\n",
    "- The feature space becomes increasingly sparse for an increasing number of.\n",
    "- KNN is very susceptible to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bfdfa1-4adb-4441-b34e-8fc9258634dd",
   "metadata": {},
   "source": [
    "### Q6. How do you handle missing values in KNN?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33440dee-80b1-47ad-834f-b9707b32c03d",
   "metadata": {},
   "source": [
    "KNN imputes missing values by finding the closest points in the dataset to the missing value. It then uses the mean value of those points to estimate the missing value. KNN imputes values more accurately without requiring as much investigation into the source of the missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7a654-2554-431b-bbdd-7d313db685f1",
   "metadata": {},
   "source": [
    "#### ways to handle missing values:\n",
    "- Imputation: Inserting a descriptive value or computing a value based on the remaining known value\n",
    "- Interpolation: Estimating unknown values by comparing them to known values\n",
    "- Multivariate imputation: Estimating missing values based on other variables using linear regression\n",
    "- MissForest: Using a Random Forest algorithm to generate better predictions at each iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3de455-47d8-4446-af3b-20fee5b6c66b",
   "metadata": {},
   "source": [
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9958c0c8-c2b6-4dc4-ab2e-d40e08c903fd",
   "metadata": {},
   "source": [
    "\n",
    "**KNN Classifier**:\n",
    "\n",
    "1. **Problem Type**: KNN classifier is used for classification problems where the goal is to categorize data points into predefined classes or categories (e.g., spam detection, image recognition, sentiment analysis).\n",
    "\n",
    "2. **Output**: The output of a KNN classifier is a discrete class label indicating the predicted category for each data point.\n",
    "\n",
    "3. **Performance Metrics**: Common evaluation metrics for KNN classifiers include accuracy, precision, recall, F1-score, confusion matrix, ROC-AUC, and others.\n",
    "\n",
    "4. **Strengths**:\n",
    "   - Simple and easy to understand.\n",
    "   - Suitable for problems with categorical or discrete target variables.\n",
    "   - Works well when the decision boundary is non-linear and complex.\n",
    "\n",
    "5. **Weaknesses**:\n",
    "   - Sensitive to the choice of hyperparameter K.\n",
    "   - Computationally expensive for large datasets.\n",
    "   - May not perform well when features are not equally important or when data is imbalanced.\n",
    "\n",
    "**KNN Regressor**:\n",
    "\n",
    "1. **Problem Type**: KNN regressor is used for regression problems where the goal is to predict a continuous numeric value (e.g., house price prediction, temperature forecasting, stock price prediction).\n",
    "\n",
    "2. **Output**: The output of a KNN regressor is a continuous numeric value, representing the predicted target value for each data point.\n",
    "\n",
    "3. **Performance Metrics**: Common evaluation metrics for KNN regressors include mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), R-squared (RÂ²), and others.\n",
    "\n",
    "4. **Strengths**:\n",
    "   - Simple and interpretable.\n",
    "   - Suitable for problems with continuous target variables.\n",
    "   - Works well when relationships between features and the target variable are non-linear.\n",
    "\n",
    "5. **Weaknesses**:\n",
    "   - Sensitive to the choice of hyperparameter K.\n",
    "   - Prone to the curse of dimensionality in high-dimensional spaces.\n",
    "   - May not perform well when relationships are highly complex or exhibit strong heteroscedasticity.\n",
    "\n",
    "**Which One to Choose**:\n",
    "\n",
    "1. **KNN Classifier** is better suited for problems where the target variable is categorical or discrete and you want to classify data into specific categories.\n",
    "\n",
    "2. **KNN Regressor** is better suited for problems where the target variable is continuous, and the goal is to predict numeric values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae936985-e177-4093-b73d-c1d348064758",
   "metadata": {},
   "source": [
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43758c93-576b-4456-929d-83257f968b24",
   "metadata": {},
   "source": [
    "**KNN Classifier**:\n",
    "\n",
    "\n",
    "**Strengths**:\n",
    "   - Simple and easy to understand.\n",
    "   - Suitable for problems with categorical or discrete target variables.\n",
    "   - Works well when the decision boundary is non-linear and complex.\n",
    "\n",
    "**Weaknesses**:\n",
    "   - Sensitive to the choice of hyperparameter K.\n",
    "   - Computationally expensive for large datasets.\n",
    "   - May not perform well when features are not equally important or when data is imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459ef3c-7645-4a5d-bc71-81338028a0f4",
   "metadata": {},
   "source": [
    "**KNN Regressor**:\n",
    "    \n",
    "\n",
    "**Strengths**:\n",
    "   - Simple and interpretable.\n",
    "   - Suitable for problems with continuous target variables.\n",
    "   - Works well when relationships between features and the target variable are non-linear.\n",
    "\n",
    "**Weaknesses**:\n",
    "   - Sensitive to the choice of hyperparameter K.\n",
    "   - Prone to the curse of dimensionality in high-dimensional spaces.\n",
    "   - May not perform well when relationships are highly complex or exhibit strong heteroscedasticity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cfb884-35f0-4628-94ae-b855ca406677",
   "metadata": {},
   "source": [
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e5c01-6c97-46ed-b3f1-d12292fdcbfc",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Euclidean Distance** (L2 Norm):\n",
    "   - Euclidean distance is also known as the L2 norm or Euclidean norm.\n",
    "   - It calculates the straight-line or \"as-the-crow-flies\" distance between two points in a multi-dimensional space. In 2D space, this is the familiar Pythagorean distance formula.\n",
    "   - The formula for Euclidean distance between two points, A and B, in n-dimensional space is:\n",
    "     de(A,B)=root(summation(Ai-Bi)**2) and i=1 to n\n",
    "   - Euclidean distance is sensitive to the magnitude of differences in each dimension and is influenced by the presence of outliers.\n",
    "\n",
    "2. **Manhattan Distance** (L1 Norm):\n",
    "   - Manhattan distance is also known as the L1 norm or taxicab distance.\n",
    "   - It measures the distance as the sum of the absolute differences between the coordinates of two points, effectively calculating the distance as if you were navigating along the grid of city streets (hence \"Manhattan\").\n",
    "   - The formula for Manhattan distance between two points, A and B, in n-dimensional space is:\n",
    "     dm(A,B)=summation(abs(Ai-Bi)) i=1 to n\n",
    "   - Manhattan distance is less sensitive to the magnitude of differences in each dimension and is often considered more robust in the presence of outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d23bd-dd7a-410e-b2b0-6d9bac4c945a",
   "metadata": {},
   "source": [
    "### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a22d90-dd82-4238-8100-d7338f124c60",
   "metadata": {},
   "source": [
    "Feature scaling is an essential preprocessing step in the K-Nearest Neighbors (KNN) algorithm and many other machine learning algorithms. Its role is to standardize or normalize the feature values in your dataset to ensure that all features contribute equally to the distance calculations. This is important because KNN relies on distance metrics (such as Euclidean distance or Manhattan distance) to determine the nearest neighbors, and the scale of features can significantly impact the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37928134-c1e3-4022-9cd1-88e33fc5dde3",
   "metadata": {},
   "source": [
    "- Equalizing Feature Influence: Without feature scaling, features with larger numeric ranges or scales can dominate the distance calculations. Features with larger values will contribute more to the distance than those with smaller values, potentially making the algorithm biased toward certain features. Feature scaling ensures that all features are on a similar scale, so they have roughly equal influence on the distance calculations.\n",
    "\n",
    "- Improved Model Performance: Scaling the features can lead to improved model performance. By reducing the impact of differences in feature scales, KNN can make better predictions. This is particularly crucial when you have features with significantly different units or scales, and you want to ensure that the KNN algorithm is sensitive to relationships in all dimensions.\n",
    "\n",
    "- Dimensionality Reduction: Feature scaling can also help mitigate the effects of the curse of dimensionality, which can make KNN computationally expensive and less effective in high-dimensional spaces. Scaling features can reduce the impact of dimensionality and make KNN more robust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
